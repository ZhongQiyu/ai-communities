class transformers.GemmaConfig

( vocab_size = 256000hidden_size = 3072intermediate_size = 24576num_hidden_layers = 28num_attention_heads = 16num_key_value_heads = 16head_dim = 256hidden_act = 'gelu_pytorch_tanh'hidden_activation = Nonemax_position_embeddings = 8192initializer_range = 0.02rms_norm_eps = 1e-06use_cache = Truepad_token_id = 0eos_token_id = 1bos_token_id = 2tie_word_embeddings = Truerope_theta = 10000.0attention_bias = Falseattention_dropout = 0.0**kwargs )
config = transformers.GemmaConfig(
    vocab_size=300000,  # 自定义词汇表大小
    hidden_size=4096,  # 自定义隐藏层大小
    num_hidden_layers=32,  # 自定义隐藏层的数量
    # 其他参数可以保持默认或根据需要进行自定义
)
